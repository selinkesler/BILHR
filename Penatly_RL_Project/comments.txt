As expected, the total reward curve has a clear upwards trend as more training epochs are performed. This shows that the model is actually learning and making better decisions over time. There are some local bottoms where the model receives less rewards than previous epochs. This can be explained by the fact that the model is exploring new states with actions that it has never seen before, and as a result it is more prone to make mistakes. However, once it finds a favorable set of actions it quickly regains the lost rewards and continues the upward trend. One interesting thing to note is that different rewards were using for when the ball goes into the goal but the goalkeeper catches it, and when the ball misses the goal completely. This is done as to not overly punish shots that are in fact aimed towards the desired general direction (the goal) but missed just because of the goalkeeper (which the model can't control).
